{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_glove_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Model...\n",
      "Done.  400000  words loaded!\n",
      "[ 0.59368   0.44825   0.5932    0.074134  0.11141   1.2793    0.16656\n",
      "  0.2407    0.39045   0.32766  -0.75034   0.35007   0.76057   0.38067\n",
      "  0.17517   0.031791  0.46849  -0.21653  -0.46282   0.39967   0.16623\n",
      " -0.011477  0.044059  0.30325   0.6153    0.47047  -0.44036  -1.5963\n",
      "  0.18433   0.23193   0.20452   0.51617   0.65734  -0.3452    0.23446\n",
      " -0.62004  -0.68741   0.28575   1.0605    0.46916  -0.85149   0.10154\n",
      "  0.21426  -0.20587   0.23636   0.21321  -0.21287   0.12107   0.18766\n",
      " -0.23282  -0.25499  -0.39631   0.84379   1.6801   -0.40941  -1.9976\n",
      " -0.69868   0.21732   1.2197    0.55126   0.44095   0.72588  -0.092053\n",
      " -0.022406  0.72039   0.1076    0.84116   0.30312  -0.42544   0.056362\n",
      "  0.13109  -0.071181 -0.10579   0.56677   0.54547   0.84113   0.14861\n",
      " -0.62628  -0.68391  -1.0831   -0.088385  0.32167   0.47794   0.091868\n",
      " -1.2559   -1.2268    0.085401  0.36833   0.081566 -0.76611   0.87751\n",
      " -0.22008   0.82401  -0.092207 -0.45941   0.46571  -0.56018  -0.54648\n",
      "  0.15162  -0.30754 ]\n"
     ]
    }
   ],
   "source": [
    "root = \"/Users/shikha/UMass/fall2017/NLP/projects\"\n",
    "dataset = root + \"/irony/SemEval2018-Task3/datasets\"\n",
    "glove_file = root + \"/glove.6B/glove.6B.100d.txt\"\n",
    "\n",
    "# Load glove file\n",
    "wordsList, wordVectors = load_glove_model(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "removing urls from tweets...\n",
      "Done.  3817  tweets loaded!\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "filename = dataset + \"/train/abc\"\n",
    "X, Y = load_dataset(filename)\n",
    "train_len = int(0.8 * X.shape[0])\n",
    "X_train = X[0:train_len]\n",
    "Y_train = Y[0:train_len]\n",
    "X_test = X[train_len:]\n",
    "Y_test = Y[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokenizer = nltk.TweetTokenizer(strip_handles=True)\n",
    "    return tokenizer.tokenize(sentence.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average no of words in a sentence =  15\n"
     ]
    }
   ],
   "source": [
    "# counting average no words in a sentence\n",
    "count = 0\n",
    "for sentence in X_train:\n",
    "    words = tokenize(sentence)\n",
    "    count += len(words)\n",
    "avg = count/train_len\n",
    "print \"Average no of words in a sentence = \", avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ids = np.zeros((X_train.shape[0], maxSeqLength), dtype='int32')\n",
    "# i = 0\n",
    "# for sentence in X_train:\n",
    "#     words = tokenize(sentence)\n",
    "#     indexCounter = 0\n",
    "#     for word in words:\n",
    "#         try:\n",
    "#             ids[i][indexCounter] = wordsList.index(word)\n",
    "#         except ValueError:\n",
    "#             ids[i][indexCounter] = 399999 #Vector for unknown words\n",
    "#         indexCounter = indexCounter + 1\n",
    "#         if indexCounter >= maxSeqLength:\n",
    "#            break\n",
    "#     i += 1\n",
    "\n",
    "# np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "lstmUnits = 12\n",
    "numClasses = 2\n",
    "iterations = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numDimensions = 100\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, reuse=tf.AUTO_REUSE)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0])-1)\n",
    "prediction = (tf.matmul(tf.cast(last, tf.float32), weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 12) (12, 2) 25\n",
      "(25, 100, 12)\n"
     ]
    }
   ],
   "source": [
    "print prediction.shape\n",
    "print last.shape, weight.shape, value.get_shape()[0]\n",
    "print value.shape\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, tf.Session().graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros((batchSize, maxSeqLength))\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,ids.shape[0]-1)\n",
    "        if Y_train[num] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "saved to models/pretrained_lstm.ckpt-10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "saved to models/pretrained_lstm.ckpt-20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "Iteration:  26\n",
      "Iteration:  27\n",
      "Iteration:  28\n",
      "Iteration:  29\n",
      "Iteration:  30\n",
      "saved to models/pretrained_lstm.ckpt-30\n",
      "Iteration:  31\n",
      "Iteration:  32\n",
      "Iteration:  33\n",
      "Iteration:  34\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print \"Training the model...\"\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    print \"Iteration: \", i\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 10 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------TEST-------------------\n",
    "\n",
    "\n",
    "# ids_test = np.zeros((X_test.shape[0], maxSeqLength), dtype='int32')\n",
    "# i = 0\n",
    "# for sentence in X_test:\n",
    "#     words = tokenize(sentence)\n",
    "#     indexCounter = 0\n",
    "#     for word in words:\n",
    "#         try:\n",
    "#             ids_test[i][indexCounter] = wordsList.index(word)\n",
    "#         except ValueError:\n",
    "#             ids_test[i][indexCounter] = 399999 #Vector for unknown words\n",
    "#         indexCounter = indexCounter + 1\n",
    "#         if indexCounter >= maxSeqLength:\n",
    "#            break\n",
    "#     i += 1\n",
    "\n",
    "# np.save('idsMatrix_Test', ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_test = np.load('idsMatrix_Test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,ids_test.shape[0]-1)\n",
    "        if Y_test[num] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids_test[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-30\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(764, 25) (764,)\n",
      "('Accuracy for this batch:', 50.999999046325684)\n",
      "('Accuracy for this batch:', 50.999999046325684)\n",
      "('Accuracy for this batch:', 40.000000596046448)\n",
      "('Accuracy for this batch:', 52.999997138977051)\n",
      "('Accuracy for this batch:', 49.000000953674316)\n",
      "('Accuracy for this batch:', 34.000000357627869)\n",
      "('Accuracy for this batch:', 46.99999988079071)\n",
      "('Accuracy for this batch:', 38.999998569488525)\n",
      "('Accuracy for this batch:', 56.999999284744263)\n",
      "('Accuracy for this batch:', 49.000000953674316)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "print ids_test.shape, X_test.shape\n",
    "acc = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "#     a = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#     acc.append(a)\n",
    "#     print(\"Loss for this batch: \", loss_val)\n",
    "#     print(\"Accuracy for this batch: \", a)\n",
    "\n",
    "# print \"Overall accuracy: \", sum(acc)/iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
